{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The fake news project by Noah Wenneberg Junge, Kevin Mark Lock and Marcus Friis-Hansen"
      ],
      "metadata": {
        "id": "joVXlprjfikz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This is our code for the fake news project. We have tried to keep as much as we could, though a small portion of specific values and small test of irrelevant classifier models and code have been lost due to editing the values/parameters or deleting/not saving it properly.\n",
        "\n",
        "We have tried to structure the document as the project linearly and also saving as many of the models and we made.\n",
        "\n",
        "Node that this document can be, but is not meant to be run all at once, and will probably result in a memory error. We made sure to run small portions of the code at a time and save the results for future loading. In the \"Setting Up\" section are all the imports and setting, to be able to run most of the sections by themselves."
      ],
      "metadata": {
        "id": "8UStYGBjCUKt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFpETDgIwvLn"
      },
      "source": [
        "\n",
        "# Setting Up\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = \"/content/drive/MyDrive/Data/\""
      ],
      "metadata": {
        "id": "PB5xNpxrS9kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMASzAXFxNtK"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_0KxsAEw1vh",
        "outputId": "5ee46411-0627-4503-99b3-8c65f395cd68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jgqhDyGwuXr"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uy0217nxw9nx"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import *\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "import pandas as pd\n",
        "from pandas import value_counts\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "from dataclasses import dataclass\n",
        "import  matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import scipy.sparse as sp\n",
        "from scipy.sparse import save_npz, load_npz\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "\n",
        "import joblib\n",
        "\n",
        "from scipy.sparse import csr_matrix, hstack, vstack\n",
        "\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4f6twf2aQLH"
      },
      "source": [
        "## Grouping labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzhPVLovaQLJ"
      },
      "outputs": [],
      "source": [
        "fakes = ['unreliable', 'fake', 'conspiracy', 'bias','junksci', 'satire','state']\n",
        "reliables = [\"reliable\", \"political\", \"clickbait\", 'caution', 'hate']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4sRBVEgaQLK"
      },
      "source": [
        "# Part 1: Data Processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_XbrkG4aQLL"
      },
      "source": [
        "### Task 1: Cleaning Functions and testing on News Sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDVocXRjaQLL"
      },
      "source": [
        "Cleaning functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-cFSs66aQLM"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "\n",
        "urlEx = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "emailEx = re.compile(r'\\\\b[\\\\w.-]+?@\\\\w+?\\\\.\\\\w{2,4}\\\\b')\n",
        "numberEx = re.compile(r'[0-9]+\\.?[0-9]+(th)?')\n",
        "dateEx = re.compile(r'\\\\b[0-9]{4}-?[0-9]{2}-?[0-9]{2}\\\\b')\n",
        "englishGrammarEx = re.compile(r'\\'(\\w+)\\b')\n",
        "nonCharEx = re.compile(r'[^<>a-zA-ZÀ-ž\\-]')\n",
        "spacesEx = re.compile(r'[\\n\\s]+')\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "#Tokenize text\n",
        "def tokenize(txt):\n",
        "    txt = str(txt) if txt is not None else \"\"\n",
        "    #tokenize\n",
        "    txt = txt.lower()\n",
        "    txt = urlEx.sub(' <URL> ', txt) # Replace URL\n",
        "    txt = emailEx.sub(' <EMAIL> ', txt) # Replace email\n",
        "    txt = numberEx.sub(' <NUM> ', txt) # Replace number\n",
        "    txt = dateEx.sub(' <DATE> ', txt) # Replace Date\n",
        "    txt = englishGrammarEx.sub (\" \", txt)# Replace dumb english grammar\n",
        "    txt = nonCharEx.sub ( \" \", txt) # Replace Any non character remaining\n",
        "    txt = spacesEx.sub ( \" \", txt) # Replace Newline and double space\n",
        "\n",
        "\n",
        "    word_tokens = txt.split()\n",
        "\n",
        "    return word_tokens\n",
        "\n",
        "#Remove stopwords\n",
        "def stopWordRemove(word_tokens):\n",
        "    #remove stop words\n",
        "    noStopWords = [i for i in word_tokens if not (i in stop_words)]\n",
        "    return noStopWords\n",
        "\n",
        "#Stem\n",
        "def stem(word_tokens):\n",
        "    #stemming\n",
        "    singles = [stemmer.stem(w) for w in word_tokens]\n",
        "    return singles\n",
        "\n",
        "# Cleandata pipeline\n",
        "def clean_data_pipeline(txt):\n",
        "    txt = tokenize(txt)\n",
        "    txt = stopWordRemove(txt)\n",
        "    txt = stem(txt)\n",
        "    return txt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4sTR7EtaQLN"
      },
      "source": [
        "Test on sample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfuDszjYaQLO"
      },
      "outputs": [],
      "source": [
        "news_sample = pd.read_csv(DATA_PATH + \"news_sample.csv\")\n",
        "cleaned = news_sample[\"content\"].progress_apply"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_kQ7EG_aQLQ"
      },
      "source": [
        "### Task 2: Explore the FakeNewsCorpus"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################## Retrieve small sample to explore\n",
        "dataChunkToExplore = pd.read_csv(DATA_PATH + \"995Data.csv\", iterator=True).get_chunk(50000)\n",
        "#dataChunkToExplore = pd.read_csv(\"0.5Data.csv\") #for testing"
      ],
      "metadata": {
        "id": "7cgEfgtxuaWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply pipline to data sample\n",
        "dataChunkToExplore[\"tokenized\"] = dataChunkToExplore[\"content\"].progress_apply(tokenize)"
      ],
      "metadata": {
        "id": "YqfQzxZ5uuVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ph8SntQZaQLR"
      },
      "outputs": [],
      "source": [
        "dataChunkToExplore[\"cleaned\"] = dataChunkToExplore[\"content\"].progress_apply(clean_data_pipeline)\n",
        "dataChunkToExplore = dataChunkToExplore[dataChunkToExplore['type'].isin(reliables)|dataChunkToExplore['type'].isin(fakes)]#remove rows with missing values or 'unknown'-label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkLNlyq4aQLR"
      },
      "source": [
        "Some useful functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3z9cDTPPaQLS"
      },
      "outputs": [],
      "source": [
        "#Make vocabluary\n",
        "def computeUniqueWords(column):\n",
        "    flattened = [i for row in column for i in row]\n",
        "    return pd.DataFrame(flattened).value_counts()\n",
        "\n",
        "#plotting function\n",
        "def plot(x, y, title, xLabel = \"Category\", yLabel = \"Count\"):\n",
        "    # Create a bar chart\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    plt.bar(x, y)\n",
        "    plt.xlabel(xLabel)\n",
        "    plt.ylabel(yLabel)\n",
        "    plt.title(title)\n",
        "    plt.xticks(rotation=90)  # Rotate x-axis labels for readability\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plotRangeWords(data : pd.DataFrame, columnName, customTxt = \"\", range=[0, 100]):\n",
        "\n",
        "    df = pd.DataFrame(computeUniqueWords(data[columnName]))\n",
        "    df.columns = [\"count\"]\n",
        "    df = df.sort_values(\"count\", ascending=False)\n",
        "    df = df.reset_index(names=[\"Category\"])\n",
        "    # Get the top 100 counts\n",
        "\n",
        "    top_in_range = df.iloc[range[0]:range[1]]  # Assuming you have more than 100 counts\n",
        "    sum = df[\"count\"].sum()\n",
        "    top_in_range.loc[:, \"count\"] = top_in_range[\"count\"].apply(lambda x: x/sum)\n",
        "\n",
        "    plot(top_in_range[\"Category\"],top_in_range['count'], f'Top {range[0]} - {range[1]} Counts for ' + str(columnName)+ f\" ({customTxt})\" )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDkAvABAaQLS"
      },
      "outputs": [],
      "source": [
        "plotRangeWords(dataChunkToExplore, \"tokenized\", range=[0, 100])\n",
        "plotRangeWords(dataChunkToExplore, \"cleaned\", range=[0, 100])\n",
        "plotRangeWords(dataChunkToExplore, \"cleaned\", range=[1, 101])\n",
        "\n",
        "tokenizedValues = computeUniqueWords(dataChunkToExplore[\"tokenized\"])\n",
        "links = tokenizedValues[\"<URL>\"] if \"<URL>\" in tokenizedValues.index else 0\n",
        "dates = tokenizedValues[\"<DATE>\"] if \"<DATE>\" in tokenizedValues.index else 0\n",
        "nums = tokenizedValues[\"<NUM>\"] if \"<NUM>\" in tokenizedValues.index else 0\n",
        "\n",
        "print(f'Number of links in all documents: \\t{links}')\n",
        "print(f'Number of dates in all documents: \\t{dates}')\n",
        "print(f'Number of numbers in all documents: \\t{nums}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ti-FU1djaQLT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plotTop10000Words(data, columnName):\n",
        "    df = pd.DataFrame(computeUniqueWords(data[columnName]))\n",
        "    df.columns = [\"count\"]\n",
        "    df = df.sort_values(\"count\", ascending=False)\n",
        "    df = df.reset_index(names=[\"Category\"])\n",
        "    # Get the top 100 counts\n",
        "    top_10000_df = df.head(10000)  # Assuming you have more than 100 counts\n",
        "\n",
        "    plot (range(1, len(top_10000_df) + 1), top_10000_df['count'], 'Top 10.000 Counts for ' + str(columnName))\n",
        "\n",
        "plotTop10000Words(dataChunkToExplore, \"tokenized\")\n",
        "plotTop10000Words(dataChunkToExplore, \"cleaned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_NmLvWbaQLT"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "fakeData = dataChunkToExplore[dataChunkToExplore[\"type\"].isin(fakes)]\n",
        "relData = dataChunkToExplore[dataChunkToExplore[\"type\"].isin(reliables)]\n",
        "plotRangeWords(fakeData, \"cleaned\", customTxt=\"fake\", range=[1, 100])\n",
        "plotRangeWords(relData, \"cleaned\", customTxt=\"rel\", range=[1, 100])\n",
        "plotRangeWords(fakeData, \"cleaned\", customTxt=\"fake\", range=[150, 250])\n",
        "plotRangeWords(relData, \"cleaned\", customTxt=\"rel\", range=[150, 250])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qN3OWieJaQLT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# make value counts\n",
        "fakeValueCounts = pd.DataFrame(computeUniqueWords(fakeData[\"cleaned\"]))\n",
        "fakeValueCounts.columns = [\"count\"]\n",
        "fakeValueCounts = fakeValueCounts.sort_values(\"count\", ascending=False)\n",
        "\n",
        "relValueCounts = pd.DataFrame(computeUniqueWords(relData[\"cleaned\"]))\n",
        "relValueCounts.columns = [\"count\"]\n",
        "relValueCounts = relValueCounts.sort_values(\"count\", ascending=False)\n",
        "\n",
        "datachunkValueCounts = pd.DataFrame(computeUniqueWords(dataChunkToExplore[\"cleaned\"]))\n",
        "datachunkValueCounts.columns = [\"count\"]\n",
        "datachunkValueCounts = datachunkValueCounts.sort_values(\"count\", ascending=False)\n",
        "\n",
        "# range to use (if range is [0, 500] it takes the top  500 words for fake plus the top 500 words for rel and conbines them, making)\n",
        "rangeUsed = [0, 500]\n",
        "data_range = pd.concat([fakeValueCounts.iloc[rangeUsed[0]:rangeUsed[1]], relValueCounts.iloc[rangeUsed[0]:rangeUsed[1]] ] )\n",
        "data_range = data_range.reset_index()\n",
        "data_range.columns = [\"word\", \"count\"]\n",
        "data_range = pd.DataFrame(data_range.groupby('word', as_index=False)['count'].sum())\n",
        "data_range = data_range.sort_values(\"count\", ascending=False)\n",
        "data_range = data_range.set_index(\"word\")\n",
        "\n",
        "\n",
        "# Since data_range is combined from two valueCount dataframes, there might be overlap between the words used. Chatgpt fix it here:\n",
        "\n",
        "\n",
        "differenceFrame = pd.DataFrame(columns = [\"word\", \"diff\", \"absDiff\"])\n",
        "for i in range(0, len(data_range)):\n",
        "    # Get the word\n",
        "\n",
        "    word = str(data_range.index[i])\n",
        "\n",
        "    # find count for each\n",
        "    fakeWordCount = fakeValueCounts.loc[word][\"count\"][0] if word in fakeValueCounts.index else 0\n",
        "    relWordCount = relValueCounts.loc[word][\"count\"][0] if word in relValueCounts.index else 0\n",
        "\n",
        "    # find difference in frequency and append it\n",
        "    difference = (1+relWordCount/relValueCounts.sum())/(1+fakeWordCount/fakeValueCounts.sum())\n",
        "    logDiff = np.log(float(difference)) # red line but works in runtime. Pylance cannot see it is a float in advance\n",
        "    newRow = pd.DataFrame({\"word\" : [word], \"diff\": [logDiff], \"absDiff\" : [abs(logDiff)]})\n",
        "    differenceFrame = pd.concat([differenceFrame, newRow])\n",
        "\n",
        "\n",
        "def plotDifferenceFrame(topN, frame, title):\n",
        "    topNFrame = differenceFrame.head(topN)\n",
        "    plot(topNFrame[\"word\"], topNFrame[\"diff\"], title, xLabel=\"Word\", yLabel=\"log(relCount / fakeCount)\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uk6lBfgPaQLU"
      },
      "outputs": [],
      "source": [
        "#Now we can make the graphs. We have decided to make 3 different graphs to see three different patters:\n",
        "differenceFrame = differenceFrame.sort_values(\"absDiff\", ascending = False)\n",
        "plotDifferenceFrame(100, differenceFrame, \"Top diferences in freq\")\n",
        "\n",
        "differenceFrame = differenceFrame.sort_values(\"diff\", ascending = False)\n",
        "plotDifferenceFrame(100, differenceFrame, \"Top words where difference is reliable-sided\")\n",
        "\n",
        "differenceFrame = differenceFrame.sort_values(\"diff\", ascending = True)\n",
        "plotDifferenceFrame(100, differenceFrame.sort_values(\"diff\", ascending = True), \"Top words where difference is fake-sided\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBYQ6FEBaQLU"
      },
      "source": [
        "When looking at the top words in these graphs there are some clear patterns. Reliable articles tend to use more words that points to a source like \"mr\", \"said\", \"newsletter\" etc and talking about stuff. They talk about stuff, that is happening without using strongly emotional words. They use words like \"court, iran, student, children\" etc, that are not so emotional. Also, reliable articles use far more numbers than fake ones, also tending to the pattern of having sources, statistics and outside knowledge.\n",
        "\n",
        "Comparing to the top words that fake articles use are very exaturated like \"nuclear\", \"parasite\", \"kill\", \"gun\" etc. These are emotional words, that probably triggers emotions in people making them likely to react in certain ways.\n",
        "\n",
        "There is a clear pattern in what words are used in each category."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boksplot of length of articles"
      ],
      "metadata": {
        "id": "GGCdVafFvCqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################## Retrieve small sample to explore\n",
        "data = pd.read_csv(DATA_PATH + \"995Data.csv\")\n",
        "#dataChunkToExplore = pd.read_csv(\"0.5Data.csv\") #for testing"
      ],
      "metadata": {
        "id": "fVOHg73vvBuo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6396d3fd-d3b4-48a5-aa9c-2650efc958f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-da1be9eea668>:2: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  data = pd.read_csv(DATA_PATH + \"995Data.csv\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply pipline to data sample\n",
        "data[\"tokenized\"] = data[\"content\"].progress_apply(tokenize)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ed7ba29-8c95-4286-aafa-30af06b5127a",
        "id": "fMuetEGFvBuv"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 995000/995000 [06:02<00:00, 2744.32it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"lengths\"] = data[\"tokenized\"].progress_apply(lambda tokens : len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKGA9Zz8vPOc",
        "outputId": "e495d70d-4b61-4663-ecbb-f78fbf188b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 995000/995000 [00:01<00:00, 962189.74it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a boxplot for the 'lengths' column\n",
        "data.boxplot(column=\"lengths\", showfliers=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "ivldz4D8w0Do",
        "outputId": "89a4d61b-84d9-4021-9723-7527ba3a9f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count of Fake and Rel"
      ],
      "metadata": {
        "id": "TqR1vt1vzV6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[data['type'].isin(reliables) | data['type'].isin(fakes)]\n",
        "print(f\"Amount of reliables: {len(data[data['type'].isin(reliables)])}\")\n",
        "print(f\"Amount of fakes: {len(data[data['type'].isin(fakes)])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tztI-xzW05zY",
        "outputId": "3fba567e-a57b-4393-963b-11f9a43920b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amount of reliables: 449273\n",
            "Amount of fakes: 397961\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnXoJmrFaQLU"
      },
      "source": [
        "### Task 3: Clean entire Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "chunksize = 500\n",
        "dataChunk = pd.DataFrame(columns=[\"type\", \"cleaned\"])\n",
        "tqdm.pandas()\n",
        "with tqdm(total=995000, desc=\"Cleaning Progress\") as pbar:\n",
        "  for i, chunk in enumerate(pd.read_csv(DATA_PATH + \"995Data.csv\", chunksize=chunksize, dtype=object)):\n",
        "      new = pd.DataFrame(columns=[\"type\", \"cleaned\"])\n",
        "      new[\"type\"] = chunk[\"type\"]\n",
        "      new['cleaned'] = chunk['content'].apply(clean_data_pipeline)\n",
        "\n",
        "      # Save the cleaned chunk if needed\n",
        "      dataChunk = pd.concat([dataChunk, new])\n",
        "      pbar.update(chunksize)\n",
        "\n",
        "try:\n",
        "    dataChunk.to_parquet(DATA_PATH +\"cleaned995DataReal.parquet\")\n",
        "    print(\"yes\")\n",
        "except:\n",
        "    print(\"no\")"
      ],
      "metadata": {
        "id": "LqyC8WnqS074"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXpqttTwaQLU"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "chunksize = 500\n",
        "dataChunk = pd.DataFrame(columns=[\"type\", \"cleaned\", \"domain\"])\n",
        "tqdm.pandas()\n",
        "with tqdm(total=995000, desc=\"Cleaning Progress\") as pbar:\n",
        "  for i, chunk in enumerate(pd.read_csv(DATA_PATH +\"995Data.csv\", chunksize=chunksize, dtype=object)):\n",
        "      new = pd.DataFrame(columns=[\"type\", \"cleaned\"])\n",
        "      new[\"type\"] = chunk[\"type\"]\n",
        "      new[\"domain\"] = chunk[\"domain\"]\n",
        "      new['cleaned'] = chunk['content'].apply(lambda txt : stopWordRemove(tokenize(txt)))\n",
        "\n",
        "      # Save the cleaned chunk if needed\n",
        "      dataChunk = pd.concat([dataChunk, new])\n",
        "      pbar.update(chunksize)\n",
        "\n",
        "try:\n",
        "    dataChunk.to_parquet(DATA_PATH + \"notStemmedWithDomain.parquet\")\n",
        "    print(\"yes\")\n",
        "except:\n",
        "    print(\"no\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaGsGwvL9DTF"
      },
      "outputs": [],
      "source": [
        "data = pd.read_parquet(DATA_PATH +\"cleaned995DataReal.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es32IqgJ_mY3"
      },
      "outputs": [],
      "source": [
        "data_notStemmed = pd.read_parquet(DATA_PATH +\"notStemmedWithDomain.parquet\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anLx39dKmq4n"
      },
      "outputs": [],
      "source": [
        "def grouping_rules(label):\n",
        "    if label in fakes:\n",
        "        return 1\n",
        "    elif label in reliables:\n",
        "        return 0\n",
        "    else:\n",
        "        raise Exception(\"Label is not in either fakes or reliables\")\n",
        "\n",
        "\n",
        "# This removes any row, that has a cleaned-list of length 5 or fewer.\n",
        "# Then it removes any row with type that we are not taking into account eg. unknown, missing etc.\n",
        "# Finaly applying grouping rule, making fake = 1 and reliable = 0\n",
        "def removeAndGroupingRule(df):\n",
        "  df = df[df['cleaned'].apply(lambda x: len(x) > 5)]\n",
        "  df = df[df['type'].isin(reliables) | df['type'].isin(fakes)]\n",
        "  df = df.reset_index(drop=True)\n",
        "  df[\"type\"] = df[\"type\"].apply(grouping_rules)\n",
        "  return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fW-kFJlZ05Ik"
      },
      "outputs": [],
      "source": [
        "data = removeAndGroupingRule(data)\n",
        "data_notStemmed = removeAndGroupingRule(data_notStemmed)\n",
        "print(len(data))\n",
        "print(len(data_notStemmed))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDxnZIEnaQLU"
      },
      "source": [
        "### Task 4 Split the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XAWfv4H_1My"
      },
      "outputs": [],
      "source": [
        "X = np.array(data[\"cleaned\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddBYDVD3AF-a"
      },
      "outputs": [],
      "source": [
        "X_notStemmed = np.array(data_notStemmed[\"cleaned\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "domains = np.array(data_notStemmed[\"domain\"])"
      ],
      "metadata": {
        "id": "H92d63Q1GUq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkK5l8xnAJiu"
      },
      "outputs": [],
      "source": [
        "y = data[\"type\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFBhk4Oz9WK_"
      },
      "outputs": [],
      "source": [
        "#first split data into training(80%) and other(20%)\n",
        "X_train, X_other, X_train_notStemmed, X_other_notStemmed, y_train, y_other, domains_train, domains_other = train_test_split(X, X_notStemmed, y, domains, random_state=0,test_size=0.20)\n",
        "\n",
        "#second split to split other into validation(10%) and test(10%)\n",
        "X_val, X_test, X_val_notStemmed, X_test_notStemmed, y_val, y_test, domains_val, domains_test = train_test_split(X_other, X_other_notStemmed, y_other, domains_other,  random_state= 0 , test_size= 0.50,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loJMID1d1wEr"
      },
      "outputs": [],
      "source": [
        "\n",
        "np.savez(\n",
        "    DATA_PATH +'dataSplitted.npz',\n",
        "    X_train=X_train,\n",
        "    X_train_notStemmed=X_train_notStemmed,\n",
        "    y_train=y_train,\n",
        "    X_val=X_val,\n",
        "    X_val_notStemmed=X_val_notStemmed,\n",
        "    y_val=y_val,\n",
        "    X_test=X_test,\n",
        "    X_test_notStemmed=X_test_notStemmed,\n",
        "    y_test=y_test\n",
        ")\n",
        "\n",
        "np.savez(\n",
        "    DATA_PATH +'domains.npz',\n",
        "    domains_train=domains_train,\n",
        "    domains_val=domains_val,\n",
        "    domains_test=domains_test,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEYhy5aLaQLV"
      },
      "source": [
        "# Part 2: Simple Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3iZ3dW4z9iS"
      },
      "source": [
        "## Make BoW vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0OWCV-kCBqU"
      },
      "outputs": [],
      "source": [
        "dataSplitted = np.load(DATA_PATH +'dataSplitted.npz', allow_pickle=True)\n",
        "X_train = dataSplitted[\"X_train\"]\n",
        "X_val = dataSplitted[\"X_val\"]\n",
        "X_test = dataSplitted[\"X_test\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMuYT1DkaQLV"
      },
      "source": [
        "For our simple vector, we will use a simple BoW vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZU-ySZKNHpdq"
      },
      "outputs": [],
      "source": [
        "X_train_joined = np.array(pd.DataFrame(X_train)[0].progress_apply(lambda tokens: ' '.join(tokens)))\n",
        "X_val_joined = np.array(pd.DataFrame(X_val)[0].progress_apply(lambda tokens: ' '.join(tokens)))\n",
        "X_test_joined = np.array(pd.DataFrame(X_test)[0].progress_apply(lambda tokens: ' '.join(tokens)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JP9uIiK9aQLV"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(min_df=0.01)\n",
        "\n",
        "vectorizer.fit(X_train_joined)\n",
        "\n",
        "bow_train = vectorizer.transform(X_train_joined)\n",
        "bow_val = vectorizer.transform(X_val_joined)\n",
        "bow_test = vectorizer.transform(X_test_joined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyItXluVDoli"
      },
      "outputs": [],
      "source": [
        "joblib.dump({\n",
        "    'bow_vectorizer' : vectorizer,\n",
        "    'bow_train': bow_train,\n",
        "    'bow_val': bow_val,\n",
        "    'bow_test': bow_test\n",
        "}, DATA_PATH +'bow.joblib')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLIRYlp60dq2"
      },
      "source": [
        "## Preprocessesing Scraped Articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bKCH_bT0PBe"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEBeH7NU9RKr"
      },
      "source": [
        "### Model 0: Forest classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FO6gMkv99RKs"
      },
      "outputs": [],
      "source": [
        "dataSplitted = np.load(DATA_PATH +'dataSplitted.npz', allow_pickle=True)\n",
        "bow = joblib.load(DATA_PATH +\"bow.joblib\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_WcCxte9RKu"
      },
      "outputs": [],
      "source": [
        "X_train = bow[\"bow_train\"]\n",
        "X_val = bow[\"bow_val\"]\n",
        "X_test = bow[\"bow_test\"]\n",
        "\n",
        "y_train = dataSplitted[\"y_train\"]\n",
        "y_val = dataSplitted[\"y_val\"]\n",
        "y_test = dataSplitted[\"y_test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2DALMz99RKv"
      },
      "outputs": [],
      "source": [
        "model0_simple = RandomForestClassifier(max_depth=20, random_state=0)\n",
        "model0_simple.fit(X_train[0: 40000], y_train[0:40000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_BPFNiz9RKv"
      },
      "outputs": [],
      "source": [
        "#make prediction\n",
        "y_pred = model0_simple.predict(X_val)\n",
        "y_pred = np.rint(y_pred)\n",
        "\n",
        "\n",
        "\n",
        "print(\"accuracy: \", accuracy_score(y_val, y_pred))\n",
        "print(\"precision: \", precision_score(y_val, y_pred, average=\"binary\"))\n",
        "print(\"recall: \", recall_score(y_val, y_pred, average=\"binary\"))\n",
        "print(\"f1: \", f1_score(y_val, y_pred, average=\"binary\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(model0_simple, DATA_PATH +\"model0_simple.joblib\")"
      ],
      "metadata": {
        "id": "niBA3CeYb-pK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nu2TyfeJ0CRM"
      },
      "source": [
        "### Model 1: Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iby2fxpiqWIa"
      },
      "outputs": [],
      "source": [
        "dataSplitted = np.load(DATA_PATH + 'dataSplitted.npz', allow_pickle=True)\n",
        "bow = joblib.load(DATA_PATH + \"bow.joblib\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RamyCABVqWIo"
      },
      "outputs": [],
      "source": [
        "X_train = bow[\"bow_train\"]\n",
        "X_val = bow[\"bow_val\"]\n",
        "X_test = bow[\"bow_test\"]\n",
        "\n",
        "y_train = dataSplitted[\"y_train\"]\n",
        "y_val = dataSplitted[\"y_val\"]\n",
        "y_test = dataSplitted[\"y_test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTfUP58pqWIq"
      },
      "outputs": [],
      "source": [
        "model1_simple = LinearRegression()\n",
        "model1_simple.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RO6zXAjtqWIt"
      },
      "outputs": [],
      "source": [
        "#make prediction\n",
        "y_pred = model1_simple.predict(X_val)\n",
        "y_pred = np.clip(np.rint(y_pred), 0, 1)\n",
        "\n",
        "\n",
        "\n",
        "print(\"accuracy: \", accuracy_score(y_val, y_pred))\n",
        "print(\"precision: \", precision_score(y_val, y_pred, average=\"binary\"))\n",
        "print(\"recall: \", recall_score(y_val, y_pred, average=\"binary\"))\n",
        "print(\"f1: \", f1_score(y_val, y_pred, average=\"binary\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(model1_simple, DATA_PATH + \"model1_simple.joblib\")"
      ],
      "metadata": {
        "id": "uJcL3SHAAh2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksS8574-0UfA"
      },
      "source": [
        "### Model 2: Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVjx1UpCLHZe"
      },
      "outputs": [],
      "source": [
        "dataSplitted = np.load(DATA_PATH +'dataSplitted.npz', allow_pickle=True)\n",
        "bow = joblib.load(DATA_PATH +\"bow.joblib\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8AR9mKyLKJS"
      },
      "outputs": [],
      "source": [
        "X_train = bow[\"bow_train\"]\n",
        "X_val = bow[\"bow_val\"]\n",
        "X_test = bow[\"bow_test\"]\n",
        "\n",
        "y_train = dataSplitted[\"y_train\"]\n",
        "y_val = dataSplitted[\"y_val\"]\n",
        "y_test = dataSplitted[\"y_test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ff_Uoxk8aQLW"
      },
      "outputs": [],
      "source": [
        "model2_simple = LogisticRegression(max_iter=300, verbose=100)\n",
        "model2_simple.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sh6cRai6aQLX"
      },
      "outputs": [],
      "source": [
        "#make prediction\n",
        "y_pred = model2_simple.predict(X_val)\n",
        "y_pred = np.rint(y_pred)\n",
        "\n",
        "print(\"accuracy: \", accuracy_score(y_val, y_pred))\n",
        "print(\"precision: \", precision_score(y_val, y_pred, average=\"binary\"))\n",
        "print(\"recall: \", recall_score(y_val, y_pred, average=\"binary\"))\n",
        "print(\"f1: \", f1_score(y_val, y_pred, average=\"binary\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(model2_simple, DATA_PATH +\"model2_simple.joblib\")"
      ],
      "metadata": {
        "id": "lZet-M_bhe8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "final test"
      ],
      "metadata": {
        "id": "gerq9enbJJyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2_simple = joblib.load(DATA_PATH + \"model2_simple.joblib\")"
      ],
      "metadata": {
        "id": "s_qWEJ8wJDGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bm3AxPrIJLW2"
      },
      "outputs": [],
      "source": [
        "#make prediction\n",
        "y_pred = model2_simple.predict(X_test)\n",
        "y_pred = np.rint(y_pred)\n",
        "\n",
        "print(\"accuracy: \", accuracy_score(y_test, y_pred))\n",
        "print(\"precision: \", precision_score(y_test, y_pred, average=\"binary\"))\n",
        "print(\"recall: \", recall_score(y_test, y_pred, average=\"binary\"))\n",
        "print(\"f1: \", f1_score(y_test, y_pred, average=\"binary\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCdgSINk0t8o"
      },
      "source": [
        "### Model 3: Adding scraped Articles"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scraped = pd.read_csv(DATA_PATH + \"BC_scraped.csv\")[[\"Content\"]]\n",
        "\n",
        "scraped.columns = [\"content\"]\n",
        "scraped[\"type\"] = 0\n",
        "scraped[\"cleaned\"] = scraped[\"content\"].progress_apply(clean_data_pipeline)"
      ],
      "metadata": {
        "id": "Fcqo56QjADLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_vectorizer = joblib.load(DATA_PATH + \"bow.joblib\")[\"bow_vectorizer\"]\n"
      ],
      "metadata": {
        "id": "-ELAAnNwCe-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scraped_joined = np.array(scraped[\"cleaned\"].progress_apply(lambda tokens: ' '.join(tokens)))\n",
        "\n",
        "bow_scraped = bow_vectorizer.transform(scraped_joined)"
      ],
      "metadata": {
        "id": "1rqjcucNCzWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump({\"data\" :  scraped, \"bow\" : bow_scraped}, DATA_PATH + \"scraped_cleaned.joblib\")"
      ],
      "metadata": {
        "id": "Uq9ZmjnpCC4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scraped = joblib.load(DATA_PATH + \"scraped_cleaned.joblib\")"
      ],
      "metadata": {
        "id": "0TGJ6CowSMwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97UNk-aySncp"
      },
      "outputs": [],
      "source": [
        "dataSplitted = np.load(DATA_PATH + 'dataSplitted.npz', allow_pickle=True)\n",
        "bow = joblib.load(DATA_PATH + \"bow.joblib\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WblMv6eSSncr"
      },
      "outputs": [],
      "source": [
        "X_train = vstack([bow[\"bow_train\"], scraped[\"bow\"]])\n",
        "X_val = bow[\"bow_val\"]\n",
        "X_test = bow[\"bow_test\"]\n",
        "\n",
        "\n",
        "y_train = np.concatenate((dataSplitted[\"y_train\"], scraped[\"data\"][\"type\"]), axis = 0)\n",
        "y_val = dataSplitted[\"y_val\"]\n",
        "y_test = dataSplitted[\"y_test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6F1e9OLLSncr"
      },
      "outputs": [],
      "source": [
        "model3_simple = LogisticRegression(max_iter=300, verbose=100)\n",
        "model3_simple.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1XdNlugSncs"
      },
      "outputs": [],
      "source": [
        "#make prediction\n",
        "y_pred = model3_simple.predict(X_val)\n",
        "y_pred = np.rint(y_pred)\n",
        "\n",
        "print(\"accuracy: \", accuracy_score(y_val, y_pred))\n",
        "print(\"precision: \", precision_score(y_val, y_pred, average=\"binary\"))\n",
        "print(\"recall: \", recall_score(y_val, y_pred, average=\"binary\"))\n",
        "print(\"f1: \", f1_score(y_val, y_pred, average=\"binary\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(model3_simple, DATA_PATH + \"model3_simple.joblib\")"
      ],
      "metadata": {
        "id": "VI5-GB8RhoVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsqceovA0oBh"
      },
      "source": [
        "### Model 4: Adding domains"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naxHCkBH0Xnr"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIyE_jifZEZ9"
      },
      "source": [
        "We have done this before so we just load it:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LV64MVsaQLW"
      },
      "source": [
        "#### One hot encoding for domain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "domains = np.load(DATA_PATH + \"domains.npz\", allow_pickle=True)\n",
        "bow = joblib.load(DATA_PATH + \"bow.joblib\")"
      ],
      "metadata": {
        "id": "GTnB37PXO_Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataSplitted = np.load(DATA_PATH + 'dataSplitted.npz', allow_pickle=True)"
      ],
      "metadata": {
        "id": "pLTpCd90P5xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7lTV2oiaQLW"
      },
      "outputs": [],
      "source": [
        "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "\n",
        "doms_train = domains[\"domains_train\"].reshape(-1, 1)\n",
        "doms_val = domains[\"domains_val\"].reshape(-1, 1)\n",
        "doms_test = domains[\"domains_test\"].reshape(-1, 1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "oheDomainFitted = ohe.fit(doms_train)\n",
        "\n",
        "\n",
        "oh_train = np.array(oheDomainFitted.transform(doms_train))\n",
        "oh_val = np.array(oheDomainFitted.transform(doms_val))\n",
        "oh_test = np.array(oheDomainFitted.transform(doms_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gP3dNAHmQ_ZH"
      },
      "outputs": [],
      "source": [
        "\n",
        "bow_train = bow[\"bow_train\"]\n",
        "bow_val = bow[\"bow_val\"]\n",
        "bow_test = bow[\"bow_test\"]\n",
        "\n",
        "y_train = dataSplitted[\"y_train\"]\n",
        "y_val = dataSplitted[\"y_val\"]\n",
        "y_test = dataSplitted[\"y_test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMaE7HMjQ_ZI"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Combine the sparse matrices using hstack\n",
        "X_train = hstack([bow_train, csr_matrix(oh_train)])\n",
        "X_val = hstack([bow_val, csr_matrix(oh_val)])\n",
        "X_test = hstack([bow_test, csr_matrix(oh_test)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WD2ICxERtvB"
      },
      "outputs": [],
      "source": [
        "model4_simple = LogisticRegression(max_iter=300, verbose=100)\n",
        "model4_simple.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xM0szhFRtvC"
      },
      "outputs": [],
      "source": [
        "#make prediction\n",
        "y_pred = model4_simple.predict(X_val)\n",
        "\n",
        "\n",
        "print(\"accuracy: \", accuracy_score(y_val, y_pred))\n",
        "print(\"precision: \", precision_score(y_val, y_pred, average=\"binary\"))\n",
        "print(\"recall: \", recall_score(y_val, y_pred, average=\"binary\"))\n",
        "print(\"f1: \", f1_score(y_val, y_pred, average=\"binary\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(model4_simple, DATA_PATH + \"model4_simple.joblib\")"
      ],
      "metadata": {
        "id": "j9xhWZIMiKG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgwSt5GDaQLW"
      },
      "source": [
        "### Simple Models\n",
        "\n",
        "Here we fit a few simple models on the data an compare what works and what doesnt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5nFwE6XaQLX"
      },
      "source": [
        "# Part 3: Advanced Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtCRsA0irBVi"
      },
      "source": [
        "## Make TFIDF Vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtEaee2CMy_j"
      },
      "outputs": [],
      "source": [
        "bow = joblib.load(DATA_PATH + \"bow.joblib\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZ7N9Iu7M4gf"
      },
      "outputs": [],
      "source": [
        "bow_train = bow[\"bow_train\"]\n",
        "bow_val = bow[\"bow_val\"]\n",
        "bow_test = bow[\"bow_test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkdVMdc-zcpu"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfTransformer()\n",
        "\n",
        "# This ensures that the IDF values only come from X_train, and data from test and validation does not leak into train data.\n",
        "vectorizer.fit(bow_train)\n",
        "\n",
        "tfidf_train = vectorizer.transform(bow_train)\n",
        "tfidf_val = vectorizer.transform(bow_val)\n",
        "tfidf_test = vectorizer.transform(bow_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FzPoy8BNY_0"
      },
      "outputs": [],
      "source": [
        "joblib.dump({\n",
        "    'tfidf_vectorizer' : vectorizer,\n",
        "    'tfidf_train': tfidf_train,\n",
        "    'tfidf_val': tfidf_val,\n",
        "    'tfidf_test': tfidf_test\n",
        "}, DATA_PATH + 'tfidf.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1: TFIDF Vector\n"
      ],
      "metadata": {
        "id": "qu8vwvaz2cM_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymawO7buYjWC"
      },
      "outputs": [],
      "source": [
        "dataSplitted = np.load(DATA_PATH + 'dataSplitted.npz', allow_pickle=True)\n",
        "tfidf = joblib.load(DATA_PATH + \"tfidf.joblib\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRmGkLVFYjWG"
      },
      "outputs": [],
      "source": [
        "X_train = tfidf[\"tfidf_train\"]\n",
        "X_val = tfidf[\"tfidf_val\"]\n",
        "X_test = tfidf[\"tfidf_test\"]\n",
        "\n",
        "y_train = dataSplitted[\"y_train\"]\n",
        "y_val = dataSplitted[\"y_val\"]\n",
        "y_test = dataSplitted[\"y_test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgSInLuTr4EQ"
      },
      "outputs": [],
      "source": [
        "model1 = LogisticRegression(max_iter=300, verbose=100)\n",
        "\n",
        "model1.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNG5OPYzr-q4"
      },
      "outputs": [],
      "source": [
        "#make prediction\n",
        "y_pred = model1.predict(X_val)\n",
        "\n",
        "print(\"accuracy: \", accuracy_score(y_val, y_pred))\n",
        "print(\"precision: \", precision_score(y_val, y_pred, average=\"binary\"))\n",
        "print(\"recall: \", recall_score(y_val, y_pred, average=\"binary\"))\n",
        "print(\"f1: \", f1_score(y_val, y_pred, average=\"binary\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final F1 test (Only Used Once)"
      ],
      "metadata": {
        "id": "Fo6Z8yJzECQ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cChn7SyIEBkK"
      },
      "outputs": [],
      "source": [
        "#make prediction\n",
        "y_pred = model1.predict(X_test)\n",
        "\n",
        "print(\"accuracy: \", accuracy_score(y_test, y_pred))\n",
        "print(\"precision: \", precision_score(y_test, y_pred, average=\"binary\"))\n",
        "print(\"recall: \", recall_score(y_test, y_pred, average=\"binary\"))\n",
        "print(\"f1: \", f1_score(y_test, y_pred, average=\"binary\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlBi6gx-Sf-W"
      },
      "outputs": [],
      "source": [
        "joblib.dump(model1, DATA_PATH + \"model1_advanced.joblib\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuq_TfKNsNo_"
      },
      "source": [
        "## Model 2: Simple Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iMWWqVrOmqw"
      },
      "outputs": [],
      "source": [
        "dataSplitted = np.load(DATA_PATH + 'dataSplitted.npz', allow_pickle=True)\n",
        "tfidf = joblib.load(DATA_PATH + \"tfidf.joblib\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdGR-O8yOmq2"
      },
      "outputs": [],
      "source": [
        "X_train = tfidf[\"tfidf_train\"]\n",
        "X_val = tfidf[\"tfidf_val\"]\n",
        "X_test = tfidf[\"tfidf_test\"]\n",
        "\n",
        "y_train = dataSplitted[\"y_train\"]\n",
        "y_val = dataSplitted[\"y_val\"]\n",
        "y_test = dataSplitted[\"y_test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZHWhWtgse22"
      },
      "outputs": [],
      "source": [
        "model2 = MLPClassifier(verbose=True, solver='adam', hidden_layer_sizes=(5, 5), random_state=1, activation=\"relu\")\n",
        "model2.n_jobs = -1\n",
        "model2.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u78fmET_svK5"
      },
      "outputs": [],
      "source": [
        "#make prediction\n",
        "y_pred = model2.predict(X_val)\n",
        "\n",
        "\n",
        "print(\"accuracy: \", accuracy_score(y_val, y_pred))\n",
        "print(\"precision: \", precision_score(y_val, y_pred, average=\"binary\"))\n",
        "print(\"recall: \", recall_score(y_val, y_pred, average=\"binary\"))\n",
        "print(\"f1: \", f1_score(y_val, y_pred, average=\"binary\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final test"
      ],
      "metadata": {
        "id": "dDqn81DjEYKE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XOwJ05VEWug"
      },
      "outputs": [],
      "source": [
        "#make prediction\n",
        "y_pred = model2.predict(X_test)\n",
        "\n",
        "\n",
        "print(\"accuracy: \", accuracy_score(y_test, y_pred))\n",
        "print(\"precision: \", precision_score(y_test, y_pred, average=\"binary\"))\n",
        "print(\"recall: \", recall_score(y_test, y_pred, average=\"binary\"))\n",
        "print(\"f1: \", f1_score(y_test, y_pred, average=\"binary\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZpgVxku3Z8e"
      },
      "outputs": [],
      "source": [
        "joblib.dump(model2, DATA_PATH + \"model2_advanced.joblib\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Rokoohs-Yx"
      },
      "source": [
        "## Model 3: Adding Nodes in layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEmUXXi1RU9P"
      },
      "outputs": [],
      "source": [
        "dataSplitted = np.load(DATA_PATH + 'dataSplitted.npz', allow_pickle=True)\n",
        "tfidf = joblib.load(DATA_PATH + \"tfidf.joblib\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wCmsBAORU9R"
      },
      "outputs": [],
      "source": [
        "X_train = tfidf[\"tfidf_train\"]\n",
        "X_val = tfidf[\"tfidf_val\"]\n",
        "X_test = tfidf[\"tfidf_test\"]\n",
        "\n",
        "y_train = dataSplitted[\"y_train\"]\n",
        "y_val = dataSplitted[\"y_val\"]\n",
        "y_test = dataSplitted[\"y_test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HogtFpgytLDu"
      },
      "outputs": [],
      "source": [
        "model3 = MLPClassifier(verbose=True, solver='adam', hidden_layer_sizes=(64, 64), random_state=1, activation=\"relu\")\n",
        "model3.n_jobs = -1\n",
        "model3.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jf-mMKXMtLDw"
      },
      "outputs": [],
      "source": [
        "y_pred = model3.predict(X_val)\n",
        "acc = accuracy_score(y_val, y_pred)\n",
        "\n",
        "print(\"accuracy: \", accuracy_score(y_val, y_pred))\n",
        "print(\"precision: \", precision_score(y_val, y_pred, average=\"binary\"))\n",
        "print(\"recall: \", recall_score(y_val, y_pred, average=\"binary\"))\n",
        "print(\"f1: \", f1_score(y_val, y_pred, average=\"binary\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "final test"
      ],
      "metadata": {
        "id": "yxgoGxd9ElQZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuzaxhI2EktD"
      },
      "outputs": [],
      "source": [
        "y_pred = model3.predict(X_test)\n",
        "acc = accuracy_score(y_val, y_pred)\n",
        "\n",
        "print(\"accuracy: \", accuracy_score(y_test, y_pred))\n",
        "print(\"precision: \", precision_score(y_test, y_pred, average=\"binary\"))\n",
        "print(\"recall: \", recall_score(y_test, y_pred, average=\"binary\"))\n",
        "print(\"f1: \", f1_score(y_test, y_pred, average=\"binary\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWUPQwbw5S7x"
      },
      "outputs": [],
      "source": [
        "joblib.dump(model3, DATA_PATH + \"model3_advanced.joblib\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrfGYkZIx9wD"
      },
      "source": [
        "## Make an Embedding vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uyk_KNkFH0w"
      },
      "outputs": [],
      "source": [
        "embeddingModel = KeyedVectors.load_word2vec_format(DATA_PATH + \"GoogleNews-vectors-negative300.bin\", binary=True, limit=1000000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4u-lFQ-F-Zm"
      },
      "outputs": [],
      "source": [
        "def documentEmbed(tokens):\n",
        "  vectorArray = [embeddingModel[i] if i in embeddingModel else np.zeros(300) for i in tokens]\n",
        "\n",
        "  mean = np.mean(vectorArray, axis=0)\n",
        "  if type(mean) is np.float64:\n",
        "    print(len(tokens))\n",
        "  return mean\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvVJ2awaTmK8"
      },
      "outputs": [],
      "source": [
        "dataSplitted = np.load(DATA_PATH + 'dataSplitted.npz', allow_pickle=True)\n",
        "X_train = dataSplitted[\"X_train_notStemmed\"]\n",
        "X_val = dataSplitted[\"X_val_notStemmed\"]\n",
        "X_test = dataSplitted[\"X_test_notStemmed\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8nxaMe4TXrY"
      },
      "outputs": [],
      "source": [
        "documentEmbeddings_train = np.vstack(pd.DataFrame(X_train)[0].progress_apply(documentEmbed).values)\n",
        "documentEmbeddings_val = np.vstack(pd.DataFrame(X_val)[0].progress_apply(documentEmbed).values)\n",
        "documentEmbeddings_test = np.vstack(pd.DataFrame(X_test)[0].progress_apply(documentEmbed).values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVJXAN0QTRU_"
      },
      "outputs": [],
      "source": [
        "\n",
        "joblib.dump({\n",
        "    'embeddings_train': documentEmbeddings_train,\n",
        "    'embeddings_val': documentEmbeddings_val,\n",
        "    'embeddings_test': documentEmbeddings_test\n",
        "}, DATA_PATH + 'embeddings.joblib')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOisDatotehF"
      },
      "source": [
        "## Model 4: Neural network with embedding vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKdn8oz2jDIT"
      },
      "outputs": [],
      "source": [
        "dataSplitted = np.load(DATA_PATH + 'dataSplitted.npz', allow_pickle=True)\n",
        "embeddings = joblib.load(DATA_PATH + \"embeddings.joblib\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDBHkQmxkPJ1"
      },
      "outputs": [],
      "source": [
        "X_train = embeddings[\"embeddings_train\"]\n",
        "X_val = embeddings[\"embeddings_val\"]\n",
        "X_test = embeddings[\"embeddings_test\"]\n",
        "\n",
        "y_train = dataSplitted[\"y_train\"]\n",
        "y_val = dataSplitted[\"y_val\"]\n",
        "y_test = dataSplitted[\"y_test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OD2p6a3rtzNK"
      },
      "outputs": [],
      "source": [
        "model4 = MLPClassifier(verbose=True, solver='adam', hidden_layer_sizes=(64, 64), random_state=1, activation=\"relu\")\n",
        "model4.n_jobs = -1\n",
        "model4.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ar-O-GnstzNL"
      },
      "outputs": [],
      "source": [
        "#make prediction\n",
        "y_pred = model4.predict(X_val)\n",
        "print(\"accuracy: \", accuracy_score(y_val, y_pred))\n",
        "print(\"precision: \", precision_score(y_val, y_pred, average=\"binary\"))\n",
        "print(\"recall: \", recall_score(y_val, y_pred, average=\"binary\"))\n",
        "print(\"f1: \", f1_score(y_val, y_pred, average=\"binary\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final test"
      ],
      "metadata": {
        "id": "lF_xM-6fE1Fe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gsBenSrEv4s"
      },
      "outputs": [],
      "source": [
        "#make prediction\n",
        "y_pred = model4.predict(X_test)\n",
        "print(\"accuracy: \", accuracy_score(y_test, y_pred))\n",
        "print(\"precision: \", precision_score(y_test, y_pred, average=\"binary\"))\n",
        "print(\"recall: \", recall_score(y_test, y_pred, average=\"binary\"))\n",
        "print(\"f1: \", f1_score(y_test, y_pred, average=\"binary\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Di9K0l65XKM"
      },
      "outputs": [],
      "source": [
        "joblib.dump(model4, DATA_PATH + \"model4_advanced.joblib\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgdcnXaOvroK"
      },
      "source": [
        "## N Average embeddings pr document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_pn5-Q5kuSR"
      },
      "outputs": [],
      "source": [
        "embeddingModel = KeyedVectors.load_word2vec_format(DATA_PATH + \"GoogleNews-vectors-negative300.bin\", binary=True, limit=1000000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ilg3u4V0vqfw"
      },
      "outputs": [],
      "source": [
        "def documentEmbedN(tokens, n):\n",
        "    tokenLength = len(tokens)\n",
        "    # Pad the token length to have a minimum length of n\n",
        "    if tokenLength < n:\n",
        "        padding = np.array([''] * (n - tokenLength))\n",
        "        tokens = np.concatenate((tokens, padding))\n",
        "        tokenLength = n\n",
        "\n",
        "    vectorArrays = []\n",
        "    for i in range(n):\n",
        "        start_index = int(i * tokenLength / n)\n",
        "        end_index = int((i + 1) * tokenLength / n)\n",
        "\n",
        "        vectorArray = [embeddingModel[token] if token in embeddingModel else np.zeros(300)\n",
        "                       for token in tokens[start_index:end_index]]\n",
        "        vectorArrays.append(vectorArray)\n",
        "\n",
        "    means = []\n",
        "    for vectorArray in vectorArrays:\n",
        "        mean = np.mean(vectorArray, axis=0)\n",
        "        means.append(mean)\n",
        "\n",
        "    documentEmbed = np.concatenate(means, axis=0)\n",
        "\n",
        "    return documentEmbed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiUQqMJgkkfT"
      },
      "outputs": [],
      "source": [
        "dataSplitted = np.load(DATA_PATH + 'dataSplitted.npz', allow_pickle=True)\n",
        "X_train = dataSplitted[\"X_train_notStemmed\"]\n",
        "X_val = dataSplitted[\"X_val_notStemmed\"]\n",
        "X_test = dataSplitted[\"X_test_notStemmed\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfCjBWfU4SHy"
      },
      "source": [
        "### 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Il81buiLkkfb"
      },
      "outputs": [],
      "source": [
        "documentEmbeddings3_train = np.vstack(pd.DataFrame(X_train)[0].progress_apply(lambda tokens : documentEmbedN(tokens , 3)).values)\n",
        "documentEmbeddings3_val = np.vstack(pd.DataFrame(X_val)[0].progress_apply(lambda tokens : documentEmbedN(tokens , 3)).values)\n",
        "documentEmbeddings3_test = np.vstack(pd.DataFrame(X_test)[0].progress_apply(lambda tokens : documentEmbedN(tokens , 3)).values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jt264KRQ6-in"
      },
      "outputs": [],
      "source": [
        "documentEmbeddings3_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ytp9mwhbkkfd"
      },
      "outputs": [],
      "source": [
        "\n",
        "joblib.dump({\n",
        "    'embeddings3_train': documentEmbeddings3_train,\n",
        "    'embeddings3_val': documentEmbeddings3_val,\n",
        "    'embeddings3_test': documentEmbeddings3_test\n",
        "}, DATA_PATH + 'embeddings3.joblib')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDx1mYSY28rB"
      },
      "source": [
        "### 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t70Pl-EO28rJ"
      },
      "outputs": [],
      "source": [
        "documentEmbeddings6_train = np.vstack(pd.DataFrame(X_train)[0].progress_apply(lambda tokens : documentEmbedN(tokens , 6)).values)\n",
        "documentEmbeddings6_val = np.vstack(pd.DataFrame(X_val)[0].progress_apply(lambda tokens : documentEmbedN(tokens , 6)).values)\n",
        "documentEmbeddings6_test = np.vstack(pd.DataFrame(X_test)[0].progress_apply(lambda tokens : documentEmbedN(tokens , 6)).values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dz9dF73228rK"
      },
      "outputs": [],
      "source": [
        "\n",
        "joblib.dump({\n",
        "    'embeddings6_train': documentEmbeddings6_train,\n",
        "    'embeddings6_val': documentEmbeddings6_val,\n",
        "    'embeddings6_test': documentEmbeddings6_test\n",
        "}, DATA_PATH + 'embeddings6.joblib')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3OEw0YYwGnr"
      },
      "source": [
        "## Model 5: Neural Network on 3-embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFWLtNVGlHAs"
      },
      "outputs": [],
      "source": [
        "dataSplitted = np.load(DATA_PATH + 'dataSplitted.npz', allow_pickle=True)\n",
        "embeddings3 = joblib.load(DATA_PATH + \"embeddings3.joblib\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9AZl0FxlHAy"
      },
      "outputs": [],
      "source": [
        "X_train = embeddings3[\"embeddings3_train\"]\n",
        "X_val = embeddings3[\"embeddings3_val\"]\n",
        "X_test = embeddings3[\"embeddings3_test\"]\n",
        "\n",
        "y_train = dataSplitted[\"y_train\"]\n",
        "y_val = dataSplitted[\"y_val\"]\n",
        "y_test = dataSplitted[\"y_test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQy4aarUwU9v"
      },
      "outputs": [],
      "source": [
        "model5 = MLPClassifier(verbose=True, solver='adam', hidden_layer_sizes=(64, 64), random_state=1, activation=\"relu\")\n",
        "model5.n_jobs = -1\n",
        "model5.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBhUrXyawU9v"
      },
      "outputs": [],
      "source": [
        "#make prediction\n",
        "y_pred = model5.predict(X_val)\n",
        "\n",
        "print(\"accuracy: \", accuracy_score(y_val, y_pred))\n",
        "print(\"precision: \", precision_score(y_val, y_pred, average=\"binary\"))\n",
        "print(\"recall: \", recall_score(y_val, y_pred, average=\"binary\"))\n",
        "print(\"f1: \", f1_score(y_val, y_pred, average=\"binary\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "final test"
      ],
      "metadata": {
        "id": "-EFjqlP8FAV6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjhBH_y3E_0u"
      },
      "outputs": [],
      "source": [
        "#make prediction\n",
        "y_pred = model5.predict(X_test)\n",
        "\n",
        "print(\"accuracy: \", accuracy_score(y_test, y_pred))\n",
        "print(\"precision: \", precision_score(y_test, y_pred, average=\"binary\"))\n",
        "print(\"recall: \", recall_score(y_test, y_pred, average=\"binary\"))\n",
        "print(\"f1: \", f1_score(y_test, y_pred, average=\"binary\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHt7DrF55Z3q"
      },
      "outputs": [],
      "source": [
        "joblib.dump(model5, DATA_PATH + \"model5_advanced.joblib\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnS6V4Tg59JX"
      },
      "source": [
        "## Model 6: Neural Network on 6-embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CU4kbfkG59Ja"
      },
      "outputs": [],
      "source": [
        "dataSplitted = np.load(DATA_PATH + 'dataSplitted.npz', allow_pickle=True)\n",
        "embeddings6 = joblib.load(DATA_PATH + \"embeddings6.joblib\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34Lu3cx259Jb"
      },
      "outputs": [],
      "source": [
        "X_train = embeddings6[\"embeddings6_train\"]\n",
        "X_val = embeddings6[\"embeddings6_val\"]\n",
        "X_test = embeddings6[\"embeddings6_test\"]\n",
        "\n",
        "y_train = dataSplitted[\"y_train\"]\n",
        "y_val = dataSplitted[\"y_val\"]\n",
        "y_test = dataSplitted[\"y_test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lecq3NTx59Jc"
      },
      "outputs": [],
      "source": [
        "model6 = MLPClassifier(verbose=True, solver='adam', hidden_layer_sizes=(64, 64), random_state=1, activation=\"relu\")\n",
        "model6.n_jobs = -1\n",
        "model6.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Gq1RHum59Je"
      },
      "outputs": [],
      "source": [
        "#make prediction\n",
        "y_pred = model6.predict(X_val)\n",
        "\n",
        "print(\"accuracy: \", accuracy_score(y_val, y_pred))\n",
        "print(\"precision: \", precision_score(y_val, y_pred, average=\"binary\"))\n",
        "print(\"recall: \", recall_score(y_val, y_pred, average=\"binary\"))\n",
        "print(\"f1: \", f1_score(y_val, y_pred, average=\"binary\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "final test"
      ],
      "metadata": {
        "id": "7GMz4RbqGMDb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQLC0zNSFmGc"
      },
      "outputs": [],
      "source": [
        "#make prediction\n",
        "y_pred = model6.predict(X_test)\n",
        "\n",
        "print(\"accuracy: \", accuracy_score(y_test, y_pred))\n",
        "print(\"precision: \", precision_score(y_test, y_pred, average=\"binary\"))\n",
        "print(\"recall: \", recall_score(y_test, y_pred, average=\"binary\"))\n",
        "print(\"f1: \", f1_score(y_test, y_pred, average=\"binary\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIOtalvD59Jf"
      },
      "outputs": [],
      "source": [
        "joblib.dump(model6, DATA_PATH + \"model6_advanced.joblib\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM_W48LJuINd"
      },
      "source": [
        "## Model 7: Merge TFIDF Features and Embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nl4GefKgnPns"
      },
      "outputs": [],
      "source": [
        "dataSplitted = np.load(DATA_PATH + 'dataSplitted.npz', allow_pickle=True)\n",
        "embeddings3 = joblib.load(DATA_PATH + \"embeddings3.joblib\")\n",
        "tfidf = joblib.load(DATA_PATH + \"tfidf.joblib\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pDvjEN7nPnu"
      },
      "outputs": [],
      "source": [
        "embeddings_train = embeddings3[\"embeddings3_train\"]\n",
        "embeddings_val = embeddings3[\"embeddings3_val\"]\n",
        "embeddings_test = embeddings3[\"embeddings3_test\"]\n",
        "\n",
        "tfidf_train = tfidf[\"tfidf_train\"]\n",
        "tfidf_val = tfidf[\"tfidf_val\"]\n",
        "tfidf_test = tfidf[\"tfidf_test\"]\n",
        "\n",
        "y_train = dataSplitted[\"y_train\"]\n",
        "y_val = dataSplitted[\"y_val\"]\n",
        "y_test = dataSplitted[\"y_test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdP0OGMy2jpb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Combine the sparse matrices using hstack\n",
        "X_train = hstack([tfidf_train, csr_matrix(embeddings_train)])\n",
        "X_val = hstack([tfidf_val, csr_matrix(embeddings_val)])\n",
        "X_test = hstack([tfidf_test, csr_matrix(embeddings_test)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETE6WAt-tgdn"
      },
      "outputs": [],
      "source": [
        "\n",
        "model7 = MLPClassifier(alpha = 0.005, max_iter=11, verbose=True, solver='adam', hidden_layer_sizes=(128, 64), random_state=1, activation=\"relu\")\n",
        "model7.n_jobs = -1\n",
        "model7.fit(X_train[0:20000], y_train[0:20000])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model7.predict(X_val)"
      ],
      "metadata": {
        "id": "qGDzcOlwAPfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "experimenting with changing recall:"
      ],
      "metadata": {
        "id": "xMsj3NmKGWVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "probas = model7.predict_proba(X_val)[:, 1]\n"
      ],
      "metadata": {
        "id": "s7gMq7MYiKfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.copy(probas)\n",
        "threshold = 0.01\n",
        "y_pred[y_pred > threshold] = 1\n",
        "y_pred[y_pred <= threshold] = 0"
      ],
      "metadata": {
        "id": "BQ1FjKuul5qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "validation test"
      ],
      "metadata": {
        "id": "Nm4WzuJiGau6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"accuracy: \", accuracy_score(y_val, y_pred))\n",
        "print(\"precision: \", precision_score(y_val, y_pred, average=\"binary\"))\n",
        "print(\"recall: \", recall_score(y_val, y_pred, average=\"binary\"))\n",
        "print(\"f1: \", f1_score(y_val, y_pred, average=\"binary\"))"
      ],
      "metadata": {
        "id": "xVLazAcUgzLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "final test"
      ],
      "metadata": {
        "id": "0j9mCfl8GFfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model7.predict(X_test)\n",
        "print(\"accuracy: \", accuracy_score(y_test, y_pred))\n",
        "print(\"precision: \", precision_score(y_test, y_pred, average=\"binary\"))\n",
        "print(\"recall: \", recall_score(y_test, y_pred, average=\"binary\"))\n",
        "print(\"f1: \", f1_score(y_test, y_pred, average=\"binary\"))"
      ],
      "metadata": {
        "id": "UBIvM16pGE5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading models"
      ],
      "metadata": {
        "id": "XNudHoGLW_8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This can be used instead of training again, just load and run the tests"
      ],
      "metadata": {
        "id": "V4MKwvzQXDkO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lieyZdfo5r7j"
      },
      "outputs": [],
      "source": [
        "joblib.dump(model7, DATA_PATH + \"model7_advanced.joblib\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZykLUcXOrjK-"
      },
      "outputs": [],
      "source": [
        "model1 = joblib.load(DATA_PATH + \"model1_advanced.joblib\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = joblib.load(DATA_PATH + \"model2_advanced.joblib\")"
      ],
      "metadata": {
        "id": "MFl5GxobW9t-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = joblib.load(DATA_PATH + \"model3_advanced.joblib\")"
      ],
      "metadata": {
        "id": "eQrjnKlZW720"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model4 = joblib.load(DATA_PATH + \"model4_advanced.joblib\")"
      ],
      "metadata": {
        "id": "YnbMwhXeW6_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model5 = joblib.load(DATA_PATH + \"model5_advanced.joblib\")"
      ],
      "metadata": {
        "id": "8e5_6hr9W5qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model6 = joblib.load(DATA_PATH + \"model6_advanced.joblib\")"
      ],
      "metadata": {
        "id": "tniKuED8W4xA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model7 = joblib.load(DATA_PATH + \"model7_advanced.joblib\")"
      ],
      "metadata": {
        "id": "7Ktz5aWMKpaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ4ITMqTrkza"
      },
      "source": [
        "# Part 4: Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LIAR DATASET"
      ],
      "metadata": {
        "id": "f-Hbof0PVJHW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d-XoS_PYNxN"
      },
      "outputs": [],
      "source": [
        "embeddingModel = KeyedVectors.load_word2vec_format(DATA_PATH + \"GoogleNews-vectors-negative300.bin\", binary=True, limit=1000000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDV0K6nKYNxP"
      },
      "outputs": [],
      "source": [
        "def documentEmbedN(tokens, n):\n",
        "    tokenLength = len(tokens)\n",
        "    # Pad the token length to have a minimum length of n\n",
        "    if tokenLength < n:\n",
        "        padding = np.array([''] * (n - tokenLength))\n",
        "        tokens = np.concatenate((tokens, padding))\n",
        "        tokenLength = n\n",
        "\n",
        "    vectorArrays = []\n",
        "    for i in range(n):\n",
        "        start_index = int(i * tokenLength / n)\n",
        "        end_index = int((i + 1) * tokenLength / n)\n",
        "\n",
        "        vectorArray = [embeddingModel[token] if token in embeddingModel else np.zeros(300)\n",
        "                       for token in tokens[start_index:end_index]]\n",
        "        vectorArrays.append(vectorArray)\n",
        "\n",
        "    means = []\n",
        "    for vectorArray in vectorArrays:\n",
        "        mean = np.mean(vectorArray, axis=0)\n",
        "        means.append(mean)\n",
        "\n",
        "    documentEmbed = np.concatenate(means, axis=0)\n",
        "\n",
        "    return documentEmbed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "liar = pd.read_csv(DATA_PATH + \"train_liar.tsv\", sep='\\t', header=None)"
      ],
      "metadata": {
        "id": "00t9eMhTS25W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "liar_falses = [\"half-true\", \"false\", \"barely-true\", \"pants-fire\"]\n",
        "liar_trues = [\"true\", \"mostly-true\"]"
      ],
      "metadata": {
        "id": "-5txiwS5U398"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "liar = liar[[1, 2]]\n",
        "liar.columns = [\"type\", \"content\"]\n"
      ],
      "metadata": {
        "id": "7SMVXv0mTUbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "liar['cleaned'] = liar['content'].progress_apply(clean_data_pipeline)\n"
      ],
      "metadata": {
        "id": "GcaFKbOrVvoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "liar['lengths'] = liar['content'].progress_apply(lambda tokens: len(tokenize(tokens)))"
      ],
      "metadata": {
        "id": "XnivkG_m1ew-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "liar[\"joined\"] = np.array(liar[\"cleaned\"].progress_apply(lambda tokens: ' '.join(tokens)))"
      ],
      "metadata": {
        "id": "Xakkoy4iJ9qG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "liar[\"joined\"]"
      ],
      "metadata": {
        "id": "QNo2LMKNWLQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grouping_rules(label):\n",
        "    if label in liar_falses:\n",
        "        return 1\n",
        "    elif label in liar_trues:\n",
        "        return 0\n",
        "    else:\n",
        "        raise Exception(\"Label is not in either fakes or reliables\")\n"
      ],
      "metadata": {
        "id": "O5kiV-2GWwtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "liar[\"type\"] = liar[\"type\"].apply(grouping_rules)"
      ],
      "metadata": {
        "id": "fIBAALR9W8Ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_vectorizer = joblib.load(DATA_PATH + \"bow.joblib\")[\"bow_vectorizer\"]\n",
        "tfidf_vectorizer = joblib.load(DATA_PATH + \"tfidf.joblib\")[\"tfidf_vectorizer\"]"
      ],
      "metadata": {
        "id": "PUXmS2_HXtsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baseline:"
      ],
      "metadata": {
        "id": "SVsh8mhSJlff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2_simple = joblib.load(DATA_PATH + \"model2_simple.joblib\")\n",
        "bow = bow_vectorizer.transform(liar[\"joined\"])"
      ],
      "metadata": {
        "id": "gzU5Poq8Jmu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make prediction\n",
        "y_pred = model2_simple.predict(bow)\n",
        "\n",
        "print(\"accuracy: \", accuracy_score(liar[\"type\"], y_pred))\n",
        "print(\"precision: \", precision_score(liar[\"type\"], y_pred, average=\"binary\"))\n",
        "print(\"recall: \", recall_score(liar[\"type\"], y_pred, average=\"binary\"))\n",
        "print(\"f1: \", f1_score(liar[\"type\"], y_pred, average=\"binary\"))"
      ],
      "metadata": {
        "id": "LLF0lVgsKGbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(liar[\"type\"], y_pred, labels = [0, 1])\n",
        "tn, fp, fn, tp = confusion_matrix(liar[\"type\"], y_pred, labels = [0, 1]).ravel()\n",
        "\n",
        "print((tp, fp))\n",
        "print((fn, tn))"
      ],
      "metadata": {
        "id": "Nkl4yfMeb89X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advanced:"
      ],
      "metadata": {
        "id": "Xgxrvjv9KypS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model7 = joblib.load(DATA_PATH + \"model7_advanced.joblib\")"
      ],
      "metadata": {
        "id": "xMRN7yiDK4yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = tfidf_vectorizer.transform(bow)\n",
        "embed3 = np.vstack(liar[\"cleaned\"].progress_apply(lambda tokens : documentEmbedN(tokens , 3)).values)"
      ],
      "metadata": {
        "id": "A2VGGW8GK7Ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = hstack([tfidf, csr_matrix(embed3)])"
      ],
      "metadata": {
        "id": "2lzdNbygLBsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make prediction\n",
        "y_pred = model7.predict(X)\n",
        "\n",
        "print(\"accuracy: \", accuracy_score(liar[\"type\"], y_pred))\n",
        "print(\"precision: \", precision_score(liar[\"type\"], y_pred, average=\"binary\"))\n",
        "print(\"recall: \", recall_score(liar[\"type\"], y_pred, average=\"binary\"))\n",
        "print(\"f1: \", f1_score(liar[\"type\"], y_pred, average=\"binary\"))"
      ],
      "metadata": {
        "id": "RZnwodoWLoM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(liar[\"type\"], y_pred, labels = [0, 1])\n",
        "tn, fp, fn, tp = confusion_matrix(liar[\"type\"], y_pred, labels = [0, 1]).ravel()\n",
        "\n",
        "print((tp, fp))\n",
        "print((fn, tn))"
      ],
      "metadata": {
        "id": "7H2ThPIHfZrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boks:"
      ],
      "metadata": {
        "id": "y8NW2-KS11un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a boxplot for the 'lengths' column\n",
        "liar.boxplot(column=\"lengths\", showfliers=False)"
      ],
      "metadata": {
        "id": "xrabpbTs1361"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "JCdgSINk0t8o",
        "SsqceovA0oBh"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
